{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们先来回归一下线性回归中的梯度下降法。\n",
    "\n",
    "梯度下降法求解的是使损失函数（误差平方和）最小的权重矩阵w。\n",
    "\n",
    "误差平方和为\n",
    "\n",
    "$J(w)=\\sum_{i=1}^m(x_iw-y_i)^2$\n",
    "\n",
    "对w求导的得到梯度\n",
    "\n",
    "$\\nabla J(w)=2X^T(Xw-y)$\n",
    "\n",
    "回归系数更新\n",
    "\n",
    "$w_{k+1}=w_k-\\rho * \\nabla J(w_k)=w_k-\\rho * 2X^T(Xw_k-y)$\n",
    "\n",
    "----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们来按照这个步骤解决逻辑回归中的梯度下降。**\n",
    "\n",
    "\n",
    "一般定义逻辑回归的损失函数为\n",
    "\n",
    "$J(w)=-\\frac{1}{m} \\sum_{i=1}^m[y_ilog{h_w(x_i)}+(1-y_i)log(1-h_w(x_i))]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中\n",
    "\n",
    "$x(i) $\t每个样本数据点在某一个特征上的值，即样本对象x的某个属性的值\n",
    "\n",
    "$y(i)\t$ 每个样本数据的所属类别标签\n",
    "\n",
    "$m$\t样本数据点的个数\n",
    "\n",
    "$h_w(x)$\t样本数据的概率密度函数，即某个数据属于1类（二分类问题）的概率\n",
    "\n",
    "$J(w)$\t代价函数，估计样本属于某类的风险程度，越小代表越有可能属于这类\n",
    "\n",
    "对损失函数求导得到w的梯度为\n",
    "\n",
    "$\\nabla J(w)=\\sum_{i=1}^m x_i^T *(f(x_iw)-y_i)=   X^T(f(Xw)-y) = X^T(sigmoid(Xw)-y)$\n",
    "\n",
    "损失函数的求梯度，你可以就按正常的函数求导来推导。\n",
    "\n",
    "其中$x_i$表示第i个样本对象（行向量），$y_i$表示第i个输出结果（行向量），$f$为激活函数，X为m个样本数据集，每行为一个对象，y为m个输出结果。\n",
    "\n",
    ">**再强调一遍，在线性回归中我们一般使用的案例都是预测一个数值型数据，在逻辑回归中代表分类的数字并不是数值型的，所以经过one-hot编码后变成了多个数值型的输出结果。**\n",
    "\n",
    "所以在逻辑回归中，梯度下降法的更新公式为\n",
    "\n",
    "$w_{k+1}=w_k-\\rho  \\nabla J(w_k)=w_k-\\rho  X^T(sigmoid(Xw_k)-y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "\n",
    "在有些教程中，喜欢用列向量表示一个样本对象x，则一个对象的输出结果y也是列向量。\n",
    "\n",
    "你看到的逻辑回归的梯度写法不相同，不过不用慌张，它只是数据的行列形式不同罢了。\n",
    "\n",
    "**我们来按照这个步骤解决softmax回归中的梯度下降。**\n",
    "\n",
    "喜欢公式的可以移步https://www.cnblogs.com/Determined22/p/6362951.html\n",
    "\n",
    "我们只对softmax回归的结果给出。\n",
    "\n",
    "在softmax回归中，梯度下降法的更新公式为\n",
    "\n",
    "$w_{k+1}=w_k-\\rho  \\nabla J(w_k)=w_k-\\rho  X^T(softmax(Xw_k)-y)$\n",
    "\n",
    "\n",
    "\n",
    "> 所以说线性回归、逻辑回归、softmax回归的梯度更新公式都是一样的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机梯度下降法\n",
    "\n",
    "\n",
    "前面的梯度下降法在每次更新回归系数(最优参数)时，都需要遍历整个数据集。如果有数十亿样本和成千上万的特征，那么该方法的计算复杂度就太高了。\n",
    "\n",
    "现在介绍一种算法，一次只用一个样本点去更新回归系数(最优参数)，这样就可以有效减少计算量了，这种方法就叫做随机梯度上升算法。\n",
    "\n",
    "在线性回归中回归系数的更新如下，其中$x_k$为第k个样本对象（行向量）。$y_k$为该对象输出的结果，为数值。\n",
    "\n",
    "$w_{k+1}=w_k-2*\\rho_kx_k^T(x_kw_k-y_k)$\n",
    "\n",
    "在逻辑回归中，每个对象的输出结果（类别）被one-hot编码成向量，随机梯度下降法的更新公式为\n",
    "\n",
    "$w_{k+1}=w_k-\\rho_kx_k^T(sigmoid(x_kw_k)-y_k)$\n",
    "\n",
    "其中$x_k$为一个样本对象（行向量），$y_k$为该对象的输出（行向量）。\n",
    "\n",
    "在softmax回归中，随机梯度下降法的更新公式为\n",
    "\n",
    "$w_{k+1}=w_k-\\rho_kx_k^T(softmax(x_kw_k)-y_k)$\n",
    "\n",
    "喜欢数学推导的朋友可以自己去百度，这里就不给出了。\n",
    "\n",
    "\n",
    "----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
