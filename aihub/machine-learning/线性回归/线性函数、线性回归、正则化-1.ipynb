{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在了解线性函数前，先来了解一下线性回归和逻辑回归的区别。\n",
    "\n",
    "线性回归为**估值而生**，逻辑回归为**分类而生**。\n",
    "\n",
    "比如我们根据根据一家企业的运营质量、盈利、运营时间等来对企业进行估值，这属于估值问题，如果存在线性关系，属于线性回归问题。\n",
    "\n",
    "再比如我们对验证码图片进行识别，将图片通过线性函数也产生了一列值，但是我们的目的是为了识别图片中究竟是数字0、1、2、3、4、5、6、7、8、9中的几。\n",
    "\n",
    "所以我们还需要将线性函数产生的这一列值进行非线性变换，变换之后的结果能用来让我们分类。\n",
    "\n",
    "这是分类问题，是逻辑回归问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在今后的文章中，数据的组成形式存在两种可能。\n",
    "\n",
    "> 1、样本数据集，每列代表一个对象。也就是说一个待测对象是一个列向量\n",
    "\n",
    "> 2、样本数据集，每行代表一个对象。也就是说待测对象是一个行向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性函数\n",
    "\n",
    "我们先在数学上理解线性函数。\n",
    "\n",
    "在线性模型中，当我们想把待测对象x向量转化为数值结果y向量的时候，我们会推导出一个函数 Y=WX+B。这个就是线性函数。\n",
    "\n",
    "其中x为输入对象，x1、x2、x3为对象的特征属性，y是输出结果，y1、y2是结果的分量。\n",
    "\n",
    "权重W确定x在预测每个y的分量时的影响。而输出结果有几个分量这是要看场景的目的。\n",
    "\n",
    "比如验证码，我如果想要每张验证码具体的值，值输出结果是1维的，如果想要输出验证码图片属于每个数字的概率，则验证码是10维的。\n",
    "\n",
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/359b388e9ce24ece2260c9d12eb07910.png)\n",
    "\n",
    "1、在很多线性回归模型中根本看不到B的存在，而仅写成Y=WX，从上图中也可以看出，这是因为B可以看成是输入对象一个值为1的属性所对应的权重。\n",
    "\n",
    "即`y1=W11*X1+W21*X2+W31*X3+1*B1`，这里Y的每个分量由输入对象每个属性线性组合得出。\n",
    "\n",
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/528898db04709dd793a30a957983ac90.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、我们可以这样理解线性函数，权重w的某一行表示输入对象每个属性对输出分量的贡献率。\n",
    "\n",
    "比如w11表示待测样本属性x1对分量y1的贡献率，w21表示待测样本属性x2对分量y1的贡献率，w12表示待测样本属性x1对分量y2的贡献率。\n",
    "\n",
    "如果我们不需要预测Y2分量，权重矩阵就不存在W12，W22，W32。我们所以如果我们想要预测的结果不需要那么多分量，只是减少了权重矩阵的维度。\n",
    "\n",
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/8f161875f5260c8b2ab6bcea35e322c4.png)\n",
    "\n",
    "3、对于线性二分类，就是找到一个直接或平面超平面将两个集合分割开。\n",
    "\n",
    "因此我们可以通过判断g(x)>0或g(x)<0进行判别。则分割面或分割线满足WX=0。所以他还是一个线性问题。\n",
    "\n",
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/840ffae24b027e7821ce6ac36e249a75.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "\n",
    "在线性函数的相关教程中比较容易混淆的就是行向量和列向量。有时你也会看到如下的写法。(你会看到Y=WX和Y=XW等不同的写法，但是表达的思想是相同的 。)\n",
    "\n",
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/081c801f612695dfa3618191a3a291ac.png)\n",
    "\n",
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/5c01fa3c390349766c3e75e775b45bf9.png)\n",
    "\n",
    "不论哪种写法，其中的x始终为一个输入对象，各分量为该对象的特征属性。\n",
    "\n",
    "只不过有些地方是以行向量的形式存在，有时是以列向量的形式存在。\n",
    "\n",
    "X是多个对象形成的样本数据集，只不过有时对象按列存在，有时按行存在。\n",
    "\n",
    "> 当使用Y=WX时，X每列为一个对象，Y的每行为该对象的输出结果。当使用Y=XW时，X的每行为一个对象，Y的每列为该对象的输出结果。\n",
    "\t> 甚至有时你会看到$Y=W^TX$或者$Y=WX^T$等各种形式的形式。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么看到的教程中输出结果y都是一个值，而不是向量？\n",
    "--------------------------\n",
    "\n",
    "我们知道输出一个值的结果时，y是对象每个属性的线性组合。\n",
    "\n",
    "当输出结果为向量y时，y的每个分量，是对象每个属性的线性组合。\n",
    "\n",
    "而对象属性组合成y 的每个分量时的权重系数是相互独立的。\n",
    "\n",
    "所以在输出结果为向量时，也就是先求出组合成y的第一个分量的系数，再求出第2个分量的组合系数。\n",
    "\n",
    "只是多做了几遍求解罢了。所以你看教程中看到的基本都是输出单个值y的情况。\n",
    "\n",
    "当输出结果y为向量时，是使用矩阵同时对多组系数进行求解，而不是使用向量一个分量一个分量的求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归\n",
    "\n",
    "对于一个的拥有m个对象，n个属性的样本数据集而言，线性回归的目的就是建立一个线性函数。\n",
    "\n",
    "能对待测对象x，预测其对应的一个或者多个输出结果。\n",
    "\n",
    "比如输入对象为一个富二代，属性为年龄、零花钱、运动时间，预测结果为身高，预测结果也可以为身高、体重。\n",
    "\n",
    "而在我们平时看到的教程中，基本以预测一维数据为主，因为对于二维结果，我们可以分两次预测一维结果来形成。\n",
    "\n",
    "所以下面的教程我们就以预测一维结果为例继续讲解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果预测的结果为一维的，即Y为一个数值，则Y=WX，其中W为[b1，w11，w12，w13]的行向量，X为[1，x1，x2，x3]列向量，公式展开便可以写成如下线性组合的样式，这也就是最简单的线性回归模型。\n",
    "\n",
    "> $$ y=b1*1+w11*x1+w12*x2+w13*x3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注：此处的X在原来x的基础上，左边又加了一列，且全是1。以此来代表B向量**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个就是一个组合问题，已知一些数据，如何求里面的未知参数，给出一个最优解。 \n",
    "\n",
    "寻找最优回归系数就是一种优化问题，我们在之前讲过一种优化问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">参考：http://blog.csdn.net/luanpeng825485697/article/details/78765923"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在今后的逻辑回归，SVM和神经网络，都是优化问题，都是问了使损失函数最小的题解。\n",
    "\n",
    "只不过在那篇文章中优化问题处理的自变量为离散值的情况，线性回归问题中的自变量取值是连续值。\n",
    "\n",
    "在线性回归中的损失函数时误差平方和。后面我们会看到其他损失函数。\n",
    "\n",
    "> 为什么求解权重W，是求使误差平方和最小的矩阵，而不是直接数学运算呢？\n",
    "\n",
    "这是因为一个线性矩阵方程，直接求解，很可能无法直接求解。有唯一解的数据集，微乎其微。基本上不存在这样一个权重矩阵满足你搜集的所有样本对象的输入输出关系。\n",
    "\n",
    "因此，需要退一步，将参数求解问题，转化为求最小误差问题，即求出一个权重矩阵，使得对样本数据集中的对象进行预测，预测的结果与真实结果之间的误差尽可能小。\n",
    "\n",
    "如果样本数据集中包含m个对象，每次预测只输出一个值，则会进行m次预测，会有m个预测结果，也就会有m个预测结果的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "> 上面的教程我们知道了线性回归的目的。那么反应在数学上是什么问题呢？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "求一个最优权值矩阵，直观上，就是使用这个权值矩阵进行预测的结果误差最小。\n",
    "\n",
    "对于包含m个对象的样本数据集，每个对象预测产生一个数值，则样本数据集就会产生m个预测值，就会产生m个误差值。\n",
    "\n",
    "一般我们采用使m个误差值的平方和最小的权值矩阵为最优权值矩阵。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（若X每列为一个对象，w为行向量，则误差平方和为）\n",
    "\n",
    "$\\sum_{i=1}^{m}(y_i-wx_i)^2$\n",
    "\n",
    "（若X每行为一个对象，w为列向量，则误差平方和为）\n",
    "\n",
    "$\\sum_{i=1}^{m}(y_i-x_iw)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个误差平方和，被称为损失函数或者错误函数或者成本函数，它用来描述线性函数不好的程度。\n",
    "\n",
    "从下图来直观理解一下线性回归优化的目标——图中线段距离（平方）的平均值，也就是最小化到分割面的距离和。 \n",
    "\n",
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/27428226d0859f8726afb015180570da.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归中为什么一般选用误差平方和作为损失函数？\n",
    "\n",
    "\n",
    "上面我教程中，我们使用误差平方和作为损失函数，用来评估模型预测结果，那么我们为啥一般选误差平方和作为损失函数呢？\n",
    "\n",
    "假设模型结果与测量值 误差满足，均值为0的高斯分布，即正态分布。这个假设是靠谱的，符合一般客观统计规律。\n",
    "\n",
    "数据x与y的条件概率：\n",
    "\n",
    "$$p(y^{(i)}|x^{(i)};\\theta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})$$\n",
    "\n",
    "若使 模型与测量数据最接近，那么其概率积就最大。\n",
    "\n",
    "概率积，就是概率密度函数的连续积，这样，就形成了一个最大似然函数估计敲打。对最大似然函数估计进行推导，就得出了求导后结果： 平方和最小公式。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 也就是说选用误差平方和作为误差函数，其实就是将误差假定为了0均值的高斯正态分布。\n",
    "\n",
    ">这也就是为什么还会存在sigmoid逻辑回归（以伯努利分布分析误差），以及softmax等一般线性回归（以指数分布分析误差）。\n"
   ]
  },
  {
   "attachments": {
    "5efdde5b-e983-43cd-8a2d-c52aab2d99a8.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 除了误差平方和还有哪些损失函数\n",
    "\n",
    "\n",
    "线性回归中采用平方和的形式，一般都是由模型条件概率的最大似然函数 概率积最大值，求导，推导出来的。\n",
    "\n",
    "统计学中，损失函数一般有以下几种：\n",
    "\n",
    "1） 0-1损失函数\n",
    "\n",
    "![XA)55{1XQ4)T{DN80_L%5U0.png](attachment:5efdde5b-e983-43cd-8a2d-c52aab2d99a8.png)\n",
    "\n",
    "2） 平方损失函数\n",
    "\n",
    "$L(Y,f(X))=(Y−f(X))^2$\n",
    "\n",
    "3） 绝对损失函数\n",
    "\n",
    "$L(Y,f(X))=|Y−f(X)|$\n",
    "\n",
    "4） 对数损失函数\n",
    "\n",
    "$L(Y,P(Y|X))=−logP(Y|X)$\n",
    "\n",
    "损失函数越小，模型就越好，而且损失函数 尽量 是一个凸函数，便于收敛计算。\n",
    "\n",
    "线性回归，采用的是平方损失函数。而逻辑回归采用的是 对数 损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型求解\n",
    "\n",
    "\n",
    "接下来，我们就以误差平方和作为损失函数，以损失函数最小为目的来求解权重矩阵。有最小二乘法，梯度下降法、随机梯度下降法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最小二乘法 ##\n",
    "\n",
    "我们的目的是求使损失函数最小的权重矩阵W。首先想到的就是数学中的求导。\n",
    "\n",
    "导数等于0的地方是极大值或极小值。最小二乘就是对损失函数求导，使导出等于0。（我们先暂且把他认为是最大值、最小值吧）\n",
    "\n",
    "下面的公式还是以每个对象只预测一个数值结果为例。\n",
    "\n",
    "有两种写法：\n",
    "\n",
    "y=wx，其中w为横向量，x为列向量，y为数值。\n",
    "\n",
    "y=xw，其中w为列向量，x为横向量，y为数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、若用X表示多个样本对象，每行为一个对象。则要写成y=Xw，其中w为列向量，x为矩阵，y为列向量，每个分量表示每个对象的预测结果。\n",
    "\n",
    "此时误差平方和对权重矩阵w求导，为下面的公式。\n",
    "\n",
    "\n",
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/e0c7b67e7c2141deeb3f13e940e755fb.png)\n",
    "\n",
    "令上述公式等于0，得到：\n",
    "\n",
    "$w=(X^TX)^{-1}X^Ty$\n",
    "\n",
    "这是当前可以估计出的w的最优解。从现有数据上估计出的w可能并不是数据中的真实w值，仅是w的一个最佳估计。\n",
    "\n",
    "值得注意的是，上述公式中包含逆矩阵，也就是说，这个方程只在逆矩阵存在的时候使用，也即是这个矩阵是一个方阵，并且其行列式不为0。\n",
    "\n",
    "也就是说最小二乘方法是一个直接的数学求解公式，不过它要求X是列满秩的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、若用X表示多个样本对象，每列为一个对象。则y=wX中，w为横向量，x为矩阵，y为横向量，每个分量表示每个对象的预测结果。\n",
    "\n",
    "此时误差平方和对权重矩阵w求导，为下面的公式。\n",
    "\n",
    "$$\\frac{d (y-wX)^T(y-wX)}{d w}$$\n",
    "\n",
    "上式等于0，得到下面的公式。\n",
    "\n",
    "$w=y^TX^T(XX^T)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降法 ##\n",
    "\n",
    "在梯度下降法和随机梯度下降法中，我们使用Y=XW的形式，是为了方便公式推导。X每行为一个样本对象。当每个对象仅预测一个值时，W为列向量。\n",
    "\n",
    "分别有梯度下降法，批梯度下降法，增量梯度下降。本质上，都是偏导数，步长/最佳学习率，更新，收敛的问题。\n",
    "\n",
    "这个算法只是最优化原理中的一个普通的方法，可以结合最优化原理来学，就容易理解了。\n",
    "\n",
    "所谓梯度也就是导数。只不过是多维空间下的导数。\n",
    "\n",
    "例如上面对权重矩阵w的求导就是w的梯度。\n",
    "\n",
    "我们知道线性回归的损失函数为：（其中$x_i$为一个样本对象，行向量）\n",
    "\n",
    "$J(w)=\\sum_{i=1}^m(x_iw-y_i)^2$\n",
    "\n",
    "对w求导的导数为（X一行为一个对象，w为列向量）\n",
    "\n",
    "$\\frac{d J(w)}{d w} = 2X^T(Xw-y)$\n",
    "\n",
    "上面这个导数就是w的梯度。\n",
    "\n",
    "$\\nabla J(w)=2X^T(Xw-y)$\n",
    "\n",
    "而所谓的梯度更新，就是在原有形式上变化一个梯度的值。其中k表示第几次迭代，其中$\\rho$是学习速率。\n",
    "\n",
    "$w_{k+1}=w_k-\\rho * \\nabla J(w_k)=w_k-\\rho * 2X^T(Xw_k-y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 画外音：还有一种算法叫做梯度上升法，就是将公式中的减号换成加号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以在下图中形象的理解梯度下降法。\n",
    "\n",
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/8371b98d6065b2ed9343dc094819914e.png)\n",
    "\n",
    "从上面的图可以看出：初始点不同，获得的最小值也不同，因此梯度下降求得的只是局部最小值；\n",
    "\n",
    "注意：下降的步伐大小非常重要，因为如果太小，则找到函数最小值的速度就很慢，如果太大，则可能出现权值矩阵求解不再收敛；\n",
    "\n",
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/a38deb2f12a5ba21c651707e06deef5a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降法 ##\n",
    "\n",
    "\n",
    "随机梯度下降法，也称为在线学习算法，它不像上面的梯度下降法，每次迭代都使用了全部的数据集，这里每次迭代仅仅使用一个样本，这种方法在神经网络中使用的较多：\n",
    "\n",
    "（其中$x_k$为第k个样本对象，为行向量。$y_k$为该对象产生的预测输出数值。$w_k$为迭代第k次产生的权重列向量w，当对新的一行对象进行预测时，总是使用最新的w。$\\rho_k$ 为第k次迭代是使用的学习速度（也叫变化步长），每次迭代是使用的学习速度都不相同。刚开始学习速度快（步长大），越到后面，速度越慢（步长越短））\n",
    "\n",
    "$w_{k+1}=w_k-2*\\rho_kx_k^T(x_kw_k-y_k)$\n",
    "\n",
    "这里需要说明的是，$\\rho_k$ 必须满足两个条件：\n",
    "\n",
    "$\\displaystyle\\sum_{k=1}^{\\infty} \\rho_k \\to\\infty$\n",
    "\n",
    "$\\displaystyle\\sum_{k=1}^{\\infty} \\rho_k^2 < \\infty$\n",
    "\n",
    "\n",
    "这个过程，其实是根据新的输入样本，对$w$的一个不断的修正的过程，一般来说，我们会把$\\rho_k$ 后面的式子称为修正量。\n",
    "\n",
    "折两个条件，保证了在迭代过程中估计的修正值，会趋近于零。因此，对于足够大的k而言，迭代会突然停止，但是，由于有第一个条件的存在，这个停止不会发生的太早，并且不会再离结果非常远的时候，就停止了迭代。\n",
    "\n",
    "上面的第二个条件保证了，对于变量的随机性噪声，噪声的累积保持有限。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**去均值和归一化**\n",
    "\n",
    "此种方法应用于梯度下降，为了加快梯度下降的执行速度；\n",
    "\n",
    "思想：将各个特征属性的值标准化，使得取值范围大致都在-1<=x<=1之间；\n",
    "\n",
    "没有经过归一化，寻找最优解的过程：\n",
    "\n",
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/bb716614a9c14f9d11eedfadaa6ae1e6.png)\n",
    "\n",
    "经过归一化，把各个特征的尺度控制在相同的范围内：\n",
    "\n",
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/4dfcbc1db537c7a832a6b8180a2842e1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "\n",
    "> 梯度下降和随机梯度下降的感想\n",
    "\n",
    "通过误差不断调整权重主要有两个途径，一个是迭代次数，一个是样本数量。\n",
    "\n",
    "以下图为例，假设只存在样本1和样本2，其中样本1会倾向于权重矩阵向左侧移动，样本2会倾向于权重矩阵向左侧移动。\n",
    "\n",
    "权重矩阵w的初始值在O1处，先使用样本1调整，w调整到O2处，再使用样本2调整，w调整到O3处。\n",
    "\n",
    "第一次迭代结束，开始进行第二次迭代，使用样本1调整，w调整到O4处，使用样本2调整，w调整到O5处，开始第三次迭代，使用样本1调整到O6处，使用样本2调整到O7处，结束。\n",
    "\n",
    "![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/a66262da8ea1ca91d29c33b810b52582.png)\n",
    "\n",
    "在这个过程我们理解到由于学习速率的存在同一个样本要迭代多次，才能充分发挥样本的作用，遍历每一个样本，才能充分利用每一个样本携带的信息。\n",
    "\n",
    "----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
