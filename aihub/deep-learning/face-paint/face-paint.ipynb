{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP2f3JoE7FzJTLLv4x3sCN8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"46f09e27a4a747af9ba10e46d0e5afed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0e1c221134d74ef1819464118474fd75","IPY_MODEL_3058c7a2e81442aea11721cc3b6b9ef8","IPY_MODEL_46468aac9f5a421cb26f70c88a6bfaa7"],"layout":"IPY_MODEL_b5a3b09da54a429299e1756b0546f9b6"}},"0e1c221134d74ef1819464118474fd75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76093ed1a2ff4f88bf01c2bf16f80ae8","placeholder":"​","style":"IPY_MODEL_946ef526028a47e1a0df312d7ffe2d0f","value":"100%"}},"3058c7a2e81442aea11721cc3b6b9ef8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a5a2d34db2341fd8a91f9efb33098fd","max":8601086,"min":0,"orientation":"horizontal","style":"IPY_MODEL_20bbd28e97b14988b843a3360f71a53f","value":8601086}},"46468aac9f5a421cb26f70c88a6bfaa7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c5524fd3230423a89769800bbf31468","placeholder":"​","style":"IPY_MODEL_de9ae01be7cd477bae796dc3cd91861e","value":" 8.20M/8.20M [00:00&lt;00:00, 39.1MB/s]"}},"b5a3b09da54a429299e1756b0546f9b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76093ed1a2ff4f88bf01c2bf16f80ae8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"946ef526028a47e1a0df312d7ffe2d0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a5a2d34db2341fd8a91f9efb33098fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20bbd28e97b14988b843a3360f71a53f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7c5524fd3230423a89769800bbf31468":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de9ae01be7cd477bae796dc3cd91861e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":161,"referenced_widgets":["46f09e27a4a747af9ba10e46d0e5afed","0e1c221134d74ef1819464118474fd75","3058c7a2e81442aea11721cc3b6b9ef8","46468aac9f5a421cb26f70c88a6bfaa7","b5a3b09da54a429299e1756b0546f9b6","76093ed1a2ff4f88bf01c2bf16f80ae8","946ef526028a47e1a0df312d7ffe2d0f","7a5a2d34db2341fd8a91f9efb33098fd","20bbd28e97b14988b843a3360f71a53f","7c5524fd3230423a89769800bbf31468","de9ae01be7cd477bae796dc3cd91861e"]},"id":"z4WJcIONY1j6","executionInfo":{"status":"ok","timestamp":1664325495029,"user_tz":-480,"elapsed":8359,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}},"outputId":"d6d0aa5b-1232-4629-b62f-55c27f6d71e5"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n","  \"You are about to download and run code from an untrusted repository. In a future release, this won't \"\n","Downloading: \"https://github.com/bryandlee/animegan2-pytorch/zipball/main\" to /root/.cache/torch/hub/main.zip\n","Downloading: \"https://github.com/bryandlee/animegan2-pytorch/raw/main/weights/face_paint_512_v2.pt\" to /root/.cache/torch/hub/checkpoints/face_paint_512_v2.pt\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/8.20M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46f09e27a4a747af9ba10e46d0e5afed"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/bryandlee_animegan2-pytorch_main\n"]}],"source":["#@title 加载项目\n","# 维护：蒋李雾龙\n","\n","import torch \n","from PIL import Image\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = torch.hub.load(\"bryandlee/animegan2-pytorch:main\", \"generator\", device=device).eval()\n","face2paint = torch.hub.load(\"bryandlee/animegan2-pytorch:main\", \"face2paint\", device=device, side_by_side=True)"]},{"cell_type":"code","source":["#@title 加载所需方法\n","\n","import os\n","import dlib\n","import collections\n","from typing import Union, List\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","\n","def get_dlib_face_detector(predictor_path: str = \"shape_predictor_68_face_landmarks.dat\"):\n","\n","    if not os.path.isfile(predictor_path):\n","        model_file = \"shape_predictor_68_face_landmarks.dat.bz2\"\n","        os.system(f\"wget http://dlib.net/files/{model_file}\")\n","        os.system(f\"bzip2 -dk {model_file}\")\n","\n","    detector = dlib.get_frontal_face_detector()\n","    shape_predictor = dlib.shape_predictor(predictor_path)\n","\n","    def detect_face_landmarks(img: Union[Image.Image, np.ndarray]):\n","        if isinstance(img, Image.Image):\n","            img = np.array(img)\n","        faces = []\n","        dets = detector(img)\n","        for d in dets:\n","            shape = shape_predictor(img, d)\n","            faces.append(np.array([[v.x, v.y] for v in shape.parts()]))\n","        return faces\n","    \n","    return detect_face_landmarks\n","\n","\n","def display_facial_landmarks(\n","    img: Image, \n","    landmarks: List[np.ndarray],\n","    fig_size=[15, 15]\n","):\n","    plot_style = dict(\n","        marker='o',\n","        markersize=4,\n","        linestyle='-',\n","        lw=2\n","    )\n","    pred_type = collections.namedtuple('prediction_type', ['slice', 'color'])\n","    pred_types = {\n","        'face': pred_type(slice(0, 17), (0.682, 0.780, 0.909, 0.5)),\n","        'eyebrow1': pred_type(slice(17, 22), (1.0, 0.498, 0.055, 0.4)),\n","        'eyebrow2': pred_type(slice(22, 27), (1.0, 0.498, 0.055, 0.4)),\n","        'nose': pred_type(slice(27, 31), (0.345, 0.239, 0.443, 0.4)),\n","        'nostril': pred_type(slice(31, 36), (0.345, 0.239, 0.443, 0.4)),\n","        'eye1': pred_type(slice(36, 42), (0.596, 0.875, 0.541, 0.3)),\n","        'eye2': pred_type(slice(42, 48), (0.596, 0.875, 0.541, 0.3)),\n","        'lips': pred_type(slice(48, 60), (0.596, 0.875, 0.541, 0.3)),\n","        'teeth': pred_type(slice(60, 68), (0.596, 0.875, 0.541, 0.4))\n","    }\n","\n","    fig = plt.figure(figsize=fig_size)\n","    ax = fig.add_subplot(1, 1, 1)\n","    ax.imshow(img)\n","    ax.axis('off')\n","\n","    for face in landmarks:\n","        for pred_type in pred_types.values():\n","            ax.plot(\n","                face[pred_type.slice, 0],\n","                face[pred_type.slice, 1],\n","                color=pred_type.color, **plot_style\n","            )\n","    plt.show()\n","\n","\n","\n","import PIL.Image\n","import PIL.ImageFile\n","import numpy as np\n","import scipy.ndimage\n","\n","\n","def align_and_crop_face(\n","    img: Image.Image,\n","    landmarks: np.ndarray,\n","    expand: float = 1.0,\n","    output_size: int = 1024, \n","    transform_size: int = 4096,\n","    enable_padding: bool = True,\n","):\n","    # Parse landmarks.\n","    # pylint: disable=unused-variable\n","    lm = landmarks\n","    lm_chin          = lm[0  : 17]  # left-right\n","    lm_eyebrow_left  = lm[17 : 22]  # left-right\n","    lm_eyebrow_right = lm[22 : 27]  # left-right\n","    lm_nose          = lm[27 : 31]  # top-down\n","    lm_nostrils      = lm[31 : 36]  # top-down\n","    lm_eye_left      = lm[36 : 42]  # left-clockwise\n","    lm_eye_right     = lm[42 : 48]  # left-clockwise\n","    lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n","    lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n","\n","    # Calculate auxiliary vectors.\n","    eye_left     = np.mean(lm_eye_left, axis=0)\n","    eye_right    = np.mean(lm_eye_right, axis=0)\n","    eye_avg      = (eye_left + eye_right) * 0.5\n","    eye_to_eye   = eye_right - eye_left\n","    mouth_left   = lm_mouth_outer[0]\n","    mouth_right  = lm_mouth_outer[6]\n","    mouth_avg    = (mouth_left + mouth_right) * 0.5\n","    eye_to_mouth = mouth_avg - eye_avg\n","\n","    # Choose oriented crop rectangle.\n","    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n","    x /= np.hypot(*x)\n","    x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n","    x *= expand\n","    y = np.flipud(x) * [-1, 1]\n","    c = eye_avg + eye_to_mouth * 0.1\n","    quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n","    qsize = np.hypot(*x) * 2\n","\n","    # Shrink.\n","    shrink = int(np.floor(qsize / output_size * 0.5))\n","    if shrink > 1:\n","        rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n","        img = img.resize(rsize, PIL.Image.ANTIALIAS)\n","        quad /= shrink\n","        qsize /= shrink\n","\n","    # Crop.\n","    border = max(int(np.rint(qsize * 0.1)), 3)\n","    crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n","    crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n","    if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n","        img = img.crop(crop)\n","        quad -= crop[0:2]\n","\n","    # Pad.\n","    pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n","    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n","    if enable_padding and max(pad) > border - 4:\n","        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n","        img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n","        h, w, _ = img.shape\n","        y, x, _ = np.ogrid[:h, :w, :1]\n","        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n","        blur = qsize * 0.02\n","        img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n","        img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n","        img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n","        quad += pad[:2]\n","\n","    # Transform.\n","    img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n","    if output_size < transform_size:\n","        img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n","\n","    return img"],"metadata":{"id":"BdtS4FgoZCeP","executionInfo":{"status":"ok","timestamp":1664325500724,"user_tz":-480,"elapsed":907,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["! pip install paddlepaddle"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0x1Sms2lUBzg","executionInfo":{"status":"ok","timestamp":1664325569151,"user_tz":-480,"elapsed":18780,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}},"outputId":"b24739be-62de-4f2c-d497-3c6f9f0e904e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting paddlepaddle\n","  Downloading paddlepaddle-2.3.2-cp37-cp37m-manylinux1_x86_64.whl (112.5 MB)\n","\u001b[K     |████████████████████████████████| 112.5 MB 56 kB/s \n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (4.4.2)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (2.23.0)\n","Collecting paddle-bfloat==0.1.7\n","  Downloading paddle_bfloat-0.1.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (394 kB)\n","\u001b[K     |████████████████████████████████| 394 kB 55.6 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (1.15.0)\n","Requirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (3.17.3)\n","Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (1.21.6)\n","Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (3.3.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (7.1.2)\n","Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from paddlepaddle) (0.8.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle) (1.24.3)\n","Installing collected packages: paddle-bfloat, paddlepaddle\n","Successfully installed paddle-bfloat-0.1.7 paddlepaddle-2.3.2\n"]}]},{"cell_type":"code","source":["!pip install paddleseg\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WVHS4twdUChW","executionInfo":{"status":"ok","timestamp":1664325589101,"user_tz":-480,"elapsed":12754,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}},"outputId":"b94b4997-885c-43b3-e778-d87d3054eda5"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting paddleseg\n","  Downloading paddleseg-2.6.0-py3-none-any.whl (321 kB)\n","\u001b[K     |████████████████████████████████| 321 kB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from paddleseg) (4.64.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from paddleseg) (1.7.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from paddleseg) (3.8.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from paddleseg) (4.6.0.66)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from paddleseg) (6.0)\n","Collecting visualdl>=2.0.0\n","  Downloading visualdl-2.4.1-py3-none-any.whl (4.9 MB)\n","\u001b[K     |████████████████████████████████| 4.9 MB 38.8 MB/s \n","\u001b[?25hCollecting sklearn\n","  Downloading sklearn-0.0.tar.gz (1.1 kB)\n","Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from paddleseg) (3.4.1)\n","Collecting bce-python-sdk\n","  Downloading bce_python_sdk-0.8.74-py3-none-any.whl (204 kB)\n","\u001b[K     |████████████████████████████████| 204 kB 43.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visualdl>=2.0.0->paddleseg) (2.23.0)\n","Collecting Flask-Babel>=1.0.0\n","  Downloading Flask_Babel-2.0.0-py3-none-any.whl (9.3 kB)\n","Requirement already satisfied: flask>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from visualdl>=2.0.0->paddleseg) (1.1.4)\n","Requirement already satisfied: protobuf>=3.11.0 in /usr/local/lib/python3.7/dist-packages (from visualdl>=2.0.0->paddleseg) (3.17.3)\n","Requirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from visualdl>=2.0.0->paddleseg) (7.1.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from visualdl>=2.0.0->paddleseg) (3.2.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from visualdl>=2.0.0->paddleseg) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from visualdl>=2.0.0->paddleseg) (1.21.6)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 36.3 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from visualdl>=2.0.0->paddleseg) (1.3.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from visualdl>=2.0.0->paddleseg) (21.3)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.1->visualdl>=2.0.0->paddleseg) (1.1.0)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.1->visualdl>=2.0.0->paddleseg) (1.0.1)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.1->visualdl>=2.0.0->paddleseg) (2.11.3)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.1->visualdl>=2.0.0->paddleseg) (7.1.2)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from Flask-Babel>=1.0.0->visualdl>=2.0.0->paddleseg) (2022.2.1)\n","Requirement already satisfied: Babel>=2.3 in /usr/local/lib/python3.7/dist-packages (from Flask-Babel>=1.0.0->visualdl>=2.0.0->paddleseg) (2.10.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.1.1->visualdl>=2.0.0->paddleseg) (2.0.1)\n","Requirement already satisfied: future>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from bce-python-sdk->visualdl>=2.0.0->paddleseg) (0.16.0)\n","Collecting pycryptodome>=3.8.0\n","  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n","\u001b[K     |████████████████████████████████| 2.3 MB 47.4 MB/s \n","\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualdl>=2.0.0->paddleseg) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualdl>=2.0.0->paddleseg) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualdl>=2.0.0->paddleseg) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualdl>=2.0.0->paddleseg) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->visualdl>=2.0.0->paddleseg) (4.1.1)\n","Requirement already satisfied: dill>=0.3.5.1 in /usr/local/lib/python3.7/dist-packages (from multiprocess->visualdl>=2.0.0->paddleseg) (0.3.5.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable->paddleseg) (4.12.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable->paddleseg) (0.2.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable->paddleseg) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visualdl>=2.0.0->paddleseg) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visualdl>=2.0.0->paddleseg) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visualdl>=2.0.0->paddleseg) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visualdl>=2.0.0->paddleseg) (3.0.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->paddleseg) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->paddleseg) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->paddleseg) (1.1.0)\n","Building wheels for collected packages: sklearn\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=b89a1fbdc309ad9a29b5b0bbbd63c735833ad9bd86e5e367da424ffcbd3ef1a8\n","  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n","Successfully built sklearn\n","Installing collected packages: pycryptodome, multiprocess, Flask-Babel, bce-python-sdk, visualdl, sklearn, paddleseg\n","Successfully installed Flask-Babel-2.0.0 bce-python-sdk-0.8.74 multiprocess-0.70.13 paddleseg-2.6.0 pycryptodome-3.15.0 sklearn-0.0 visualdl-2.4.1\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/PaddlePaddle/PaddleSeg.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uekKYt1jUCje","executionInfo":{"status":"ok","timestamp":1664325608667,"user_tz":-480,"elapsed":17077,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}},"outputId":"e298a62c-f6be-47a2-812f-e16c28a7f9fa"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'PaddleSeg'...\n","remote: Enumerating objects: 20557, done.\u001b[K\n","remote: Counting objects: 100% (8/8), done.\u001b[K\n","remote: Compressing objects: 100% (8/8), done.\u001b[K\n","remote: Total 20557 (delta 2), reused 1 (delta 0), pack-reused 20549\u001b[K\n","Receiving objects: 100% (20557/20557), 345.93 MiB | 24.02 MiB/s, done.\n","Resolving deltas: 100% (13398/13398), done.\n"]}]},{"cell_type":"code","source":["%cd PaddleSeg/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dTrIgetVUCly","executionInfo":{"status":"ok","timestamp":1664325625547,"user_tz":-480,"elapsed":371,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}},"outputId":"3a1cb387-3f33-4e16-a7a1-671c56952189"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/PaddleSeg\n"]}]},{"cell_type":"code","source":["%cd contrib/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m5Rp6mj8UCn_","executionInfo":{"status":"ok","timestamp":1664325626951,"user_tz":-480,"elapsed":3,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}},"outputId":"d9def58e-3721-4bee-8993-d87da4465887"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/PaddleSeg/contrib\n"]}]},{"cell_type":"code","source":["%cd PP-HumanSeg/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u2Re_7WLUCqQ","executionInfo":{"status":"ok","timestamp":1664325628769,"user_tz":-480,"elapsed":3,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}},"outputId":"bf97b6c0-f8b3-4cbf-b127-b46a122cdfdb"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/PaddleSeg/contrib/PP-HumanSeg\n"]}]},{"cell_type":"code","source":["# 执行以下脚本下载所有Inference Model\n","!python src/download_inference_models.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OEwR0i14UCso","executionInfo":{"status":"ok","timestamp":1664325703033,"user_tz":-480,"elapsed":72282,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}},"outputId":"a5465b01-9675-441a-d1ec-ab3440043be7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Connecting to https://paddleseg.bj.bcebos.com/dygraph/pp_humanseg_v2/portrait_pp_humansegv1_lite_398x224_inference_model_with_softmax.zip\n","Downloading portrait_pp_humansegv1_lite_398x224_inference_model_with_softmax.zip\n","[==================================================] 100.00%\n","Uncompress portrait_pp_humansegv1_lite_398x224_inference_model_with_softmax.zip\n","[==================================================] 100.00%\n","Connecting to https://paddleseg.bj.bcebos.com/dygraph/pp_humanseg_v2/portrait_pp_humansegv2_lite_256x144_smaller/portrait_pp_humansegv2_lite_256x144_inference_model_with_softmax.zip\n","Downloading portrait_pp_humansegv2_lite_256x144_inference_model_with_softmax.zip\n","[==================================================] 100.00%\n","Uncompress portrait_pp_humansegv2_lite_256x144_inference_model_with_softmax.zip\n","[==================================================] 100.00%\n","Connecting to https://paddleseg.bj.bcebos.com/dygraph/pp_humanseg_v2/human_pp_humansegv1_lite_192x192_inference_model_with_softmax.zip\n","Downloading human_pp_humansegv1_lite_192x192_inference_model_with_softmax.zip\n","[==================================================] 100.00%\n","Uncompress human_pp_humansegv1_lite_192x192_inference_model_with_softmax.zip\n","[==================================================] 100.00%\n","Connecting to https://paddleseg.bj.bcebos.com/dygraph/pp_humanseg_v2/human_pp_humansegv2_lite_192x192_inference_model_with_softmax.zip\n","Downloading human_pp_humansegv2_lite_192x192_inference_model_with_softmax.zip\n","[==================================================] 100.00%\n","Uncompress human_pp_humansegv2_lite_192x192_inference_model_with_softmax.zip\n","[==================================================] 100.00%\n","Connecting to https://paddleseg.bj.bcebos.com/dygraph/pp_humanseg_v2/human_pp_humansegv1_mobile_192x192_inference_model_with_softmax.zip\n","Downloading human_pp_humansegv1_mobile_192x192_inference_model_with_softmax.zip\n","[==================================================] 100.00%\n","Uncompress human_pp_humansegv1_mobile_192x192_inference_model_with_softmax.zip\n","[==================================================] 100.00%\n","Connecting to https://paddleseg.bj.bcebos.com/dygraph/pp_humanseg_v2/human_pp_humansegv2_mobile_192x192_inference_model_with_softmax.zip\n","Downloading human_pp_humansegv2_mobile_192x192_inference_model_with_softmax.zip\n","[==================================================] 100.00%\n","Uncompress human_pp_humansegv2_mobile_192x192_inference_model_with_softmax.zip\n","[==================================================] 100.00%\n","Download inference models finished.\n"]}]},{"cell_type":"code","source":["# 可选\n","# 下载测试数据集\n","!python src/download_data.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sp1SpEOUCvB","executionInfo":{"status":"ok","timestamp":1664325930050,"user_tz":-480,"elapsed":100116,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}},"outputId":"8933c381-74c9-4f99-f5ce-4455515d3f64"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Connecting to https://paddleseg.bj.bcebos.com/humanseg/data/mini_supervisely.zip\n","Downloading mini_supervisely.zip\n","[==================================================] 100.00%\n","Uncompress mini_supervisely.zip\n","[==================================================] 100.00%\n","Connecting to https://paddleseg.bj.bcebos.com/dygraph/pp_humanseg_v2/videos.zip\n","Downloading videos.zip\n","[==================================================] 100.00%\n","Uncompress videos.zip\n","[==================================================] 100.00%\n","Connecting to https://paddleseg.bj.bcebos.com/dygraph/pp_humanseg_v2/images.zip\n","Downloading images.zip\n","[==================================================] 100.00%\n","Uncompress images.zip\n","[==================================================] 100.00%\n","Data download finished!\n"]}]},{"cell_type":"code","source":["%cd src"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WaQ54aEHY1f3","executionInfo":{"status":"ok","timestamp":1664325940485,"user_tz":-480,"elapsed":367,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}},"outputId":"a7eb2247-5b3d-4ea6-c506-3daa885df61e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/PaddleSeg/contrib/PP-HumanSeg/src\n"]}]},{"cell_type":"code","source":["import argparse\n","import os\n","import sys\n","\n","import cv2\n","import numpy as np\n","from tqdm import tqdm\n","\n","# __dir__ = os.path.dirname(os.path.abspath(__file__))\n","# sys.path.append(os.path.abspath(os.path.join(__dir__, '../../../')))\n","from paddleseg.utils import get_sys_env, logger, get_image_list\n","\n","import codecs\n","import os\n","import sys\n","import time\n","\n","import yaml\n","import numpy as np\n","import cv2\n","import paddle\n","from paddle.inference import create_predictor, PrecisionType\n","from paddle.inference import Config as PredictConfig\n","\n","\n","import paddleseg.transforms as T\n","from paddleseg.core.infer import reverse_transform\n","from paddleseg.cvlibs import manager\n","from paddleseg.utils import TimeAverager\n","\n","from optic_flow_process import optic_flow_process\n","\n","from PIL import Image\n","\n","\n","def get_bg_img(bg_img_path, img_shape):\n","    if bg_img_path is None:\n","        bg = 255 * np.ones(img_shape)\n","    elif not os.path.exists(bg_img_path):\n","        raise Exception('The --bg_img_path is not existed: {}'.format(\n","            bg_img_path))\n","    else:\n","        bg = cv2.imread(bg_img_path)\n","    return bg\n","\n","\n","def makedirs(save_dir):\n","    dirname = save_dir if os.path.isdir(save_dir) else \\\n","        os.path.dirname(save_dir)\n","    if not os.path.exists(dirname):\n","        os.makedirs(dirname)\n","\n","\n","def seg_image(args):\n","    print(args)\n","    assert os.path.exists(args['img_path']), \\\n","        \"The --img_path is not existed: {}.\".format(args['img_path'])\n","\n","    logger.info(\"Input: image\")\n","    logger.info(\"Create predictor...\")\n","    predictor = Predictor(args)\n","\n","    logger.info(\"Start predicting...\")\n","    img = cv2.imread(args['img_path'])\n","    bg_img = get_bg_img(args['bg_img_path'], img.shape)\n","    out_img = predictor.run(img, bg_img)\n","    cv2.imwrite(args['save_dir'], out_img)\n","    # im = Image.open(args['save_dir'])  \n","    # display(im)\n","\n","class Predictor:\n","    def __init__(self, args):\n","        self.args = args\n","        self.cfg = DeployConfig(args['config'], False)\n","        self.compose = T.Compose(self.cfg.transforms)\n","\n","        pred_cfg = PredictConfig(self.cfg.model, self.cfg.params)\n","        pred_cfg.disable_glog_info()\n","        if self.args['use_gpu']:\n","            pred_cfg.enable_use_gpu(100, 0)\n","\n","        self.predictor = create_predictor(pred_cfg)\n","        if self.args['test_speed']:\n","            self.cost_averager = TimeAverager()\n","\n","        if args['use_optic_flow']:\n","\n","            self.disflow = cv2.DISOpticalFlow_create(\n","                cv2.DISOPTICAL_FLOW_PRESET_ULTRAFAST)\n","            width, height = self.cfg.target_size()\n","            self.prev_gray = np.zeros((height, width), np.uint8)\n","            self.prev_cfd = np.zeros((height, width), np.float32)\n","            self.is_first_frame = True\n","\n","    def run(self, img, bg):\n","        input_names = self.predictor.get_input_names()\n","        input_handle = self.predictor.get_input_handle(input_names[0])\n","\n","        data = self.compose({'img': img})\n","        input_data = np.array([data['img']])\n","\n","        input_handle.reshape(input_data.shape)\n","        input_handle.copy_from_cpu(input_data)\n","        if self.args['test_speed']:\n","            start = time.time()\n","\n","        self.predictor.run()\n","\n","        if self.args['test_speed']:\n","            self.cost_averager.record(time.time() - start)\n","        output_names = self.predictor.get_output_names()\n","        output_handle = self.predictor.get_output_handle(output_names[0])\n","        output = output_handle.copy_to_cpu()\n","\n","        return self.postprocess(output, img, data, bg)\n","\n","    def postprocess(self, pred_img, origin_img, data, bg):\n","        trans_info = data['trans_info']\n","        score_map = pred_img[0, 1, :, :]\n","\n","        # post process\n","        if self.args['use_post_process']:\n","            mask_original = score_map.copy()\n","            mask_original = (mask_original * 255).astype(\"uint8\")\n","            _, mask_thr = cv2.threshold(mask_original, 240, 1,\n","                                        cv2.THRESH_BINARY)\n","            kernel_erode = cv2.getStructuringElement(cv2.MORPH_CROSS, (5, 5))\n","            kernel_dilate = cv2.getStructuringElement(cv2.MORPH_CROSS, (25, 25))\n","            mask_erode = cv2.erode(mask_thr, kernel_erode)\n","            mask_dilate = cv2.dilate(mask_erode, kernel_dilate)\n","            score_map *= mask_dilate\n","\n","        # optical flow\n","        if self.args['use_optic_flow']:\n","            score_map = 255 * score_map\n","            cur_gray = cv2.cvtColor(origin_img, cv2.COLOR_BGR2GRAY)\n","            cur_gray = cv2.resize(cur_gray,\n","                                  (pred_img.shape[-1], pred_img.shape[-2]))\n","            optflow_map = optic_flow_process(cur_gray, score_map, self.prev_gray, self.prev_cfd, \\\n","                    self.disflow, self.is_first_frame)\n","            self.prev_gray = cur_gray.copy()\n","            self.prev_cfd = optflow_map.copy()\n","            self.is_first_frame = False\n","            score_map = optflow_map / 255.\n","\n","        score_map = score_map[np.newaxis, np.newaxis, ...]\n","        score_map = reverse_transform(\n","            paddle.to_tensor(score_map), trans_info, mode='bilinear')\n","        alpha = np.transpose(score_map.numpy().squeeze(1), [1, 2, 0])\n","\n","        h, w, _ = origin_img.shape\n","        bg = cv2.resize(bg, (w, h))\n","        if bg.ndim == 2:\n","            bg = bg[..., np.newaxis]\n","\n","        out = (alpha * origin_img + (1 - alpha) * bg).astype(np.uint8)\n","        return out\n","\n","class DeployConfig:\n","    def __init__(self, path, vertical_screen):\n","        with codecs.open(path, 'r', 'utf-8') as file:\n","            self.dic = yaml.load(file, Loader=yaml.FullLoader)\n","\n","            [width, height] = self.dic['Deploy']['transforms'][0]['target_size']\n","            if vertical_screen and width > height:\n","                self.dic['Deploy']['transforms'][0][\n","                    'target_size'] = [height, width]\n","\n","        self._transforms = self._load_transforms(self.dic['Deploy'][\n","            'transforms'])\n","        self._dir = os.path.dirname(path)\n","\n","    @property\n","    def transforms(self):\n","        return self._transforms\n","\n","    @property\n","    def model(self):\n","        return os.path.join(self._dir, self.dic['Deploy']['model'])\n","\n","    @property\n","    def params(self):\n","        return os.path.join(self._dir, self.dic['Deploy']['params'])\n","\n","    def target_size(self):\n","        [width, height] = self.dic['Deploy']['transforms'][0]['target_size']\n","        return [width, height]\n","\n","    def _load_transforms(self, t_list):\n","        com = manager.TRANSFORMS\n","        transforms = []\n","        for t in t_list:\n","            ctype = t.pop('type')\n","            transforms.append(com[ctype](**t))\n","\n","        return transforms\n","\n","\n","\n","\n"],"metadata":{"id":"Lgw48SeQUCxO","executionInfo":{"status":"ok","timestamp":1664325945374,"user_tz":-480,"elapsed":2571,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["args = {'config': '/content/PaddleSeg/contrib/PP-HumanSeg/inference_models/portrait_pp_humansegv2_lite_256x144_inference_model_with_softmax/deploy.yaml','img_path': '/content/PaddleSeg/contrib/PP-HumanSeg/data/images/human.jpg','bg_img_path': '/content/PaddleSeg/contrib/PP-HumanSeg/data/images/bg_1.jpg','save_dir': '/content/PaddleSeg/contrib/PP-HumanSeg/data/1.jpg','use_gpu': True,'test_speed' : False,'use_optic_flow' : False,'use_post_process' : False}"],"metadata":{"id":"DKM2Y3IWUCz8","executionInfo":{"status":"ok","timestamp":1664326034479,"user_tz":-480,"elapsed":2,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["seg_image(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fiTsjsZEeDP8","executionInfo":{"status":"ok","timestamp":1664326036816,"user_tz":-480,"elapsed":681,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}},"outputId":"fb63321a-335e-434b-c0e4-e2255a2cc4cb"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["{'config': '/content/PaddleSeg/contrib/PP-HumanSeg/inference_models/portrait_pp_humansegv2_lite_256x144_inference_model_with_softmax/deploy.yaml', 'img_path': '/content/PaddleSeg/contrib/PP-HumanSeg/data/images/human.jpg', 'bg_img_path': '/content/PaddleSeg/contrib/PP-HumanSeg/data/images/bg_1.jpg', 'save_dir': '/content/PaddleSeg/contrib/PP-HumanSeg/data/1.jpg', 'use_gpu': True, 'test_speed': False, 'use_optic_flow': False, 'use_post_process': False}\n","2022-09-28 00:47:16 [INFO]\tInput: image\n","2022-09-28 00:47:16 [INFO]\tCreate predictor...\n","2022-09-28 00:47:17 [INFO]\tStart predicting...\n"]}]},{"cell_type":"code","source":["import requests\n","\n","# 加载网络或本地文件\n","img = Image.open(args['save_dir']).convert(\"RGB\")\n","# img = Image.open(\"/content/sample.jpg\").convert(\"RGB\")\n","\n","face_detector = get_dlib_face_detector()\n","landmarks = face_detector(img)\n","for landmark in landmarks:\n","    face = align_and_crop_face(img, landmark, expand=1.3)\n","    p_face = face2paint(model=model, img=face, size=512)\n","    # display(p_face)\n","    # p_face.save('1.png') # 此输出为对比图片\n","    # 裁剪为需要的部分输出\n","    x_, y_ = p_face.size\n","    out = p_face.crop((int(x_/2), 0, x_, y_))\n","    # display(out)\n","    # out.save('1.png')"],"metadata":{"id":"QEK24X44ZJ4X","executionInfo":{"status":"ok","timestamp":1664326062275,"user_tz":-480,"elapsed":22596,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["%pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"uV8IyX__2v9V","executionInfo":{"status":"ok","timestamp":1664326078810,"user_tz":-480,"elapsed":13,"user":{"displayName":"shing LIN","userId":"15916362134103428022"}},"outputId":"14e52fa3-21f0-40bc-fe6a-4d0f8442b2ea"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/PaddleSeg/contrib/PP-HumanSeg/src'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]}]}